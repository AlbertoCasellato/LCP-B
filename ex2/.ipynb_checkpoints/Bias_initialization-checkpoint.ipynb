{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1cdb964-2650-4ccd-884d-da5ad1b2a4b0",
   "metadata": {},
   "source": [
    "Restricted Boltzmann Machines (RBM) are a powerful kind of generative models designed to accomplish training processes relatively fast. In RBMs, a set of binary visible units $i$ of state $v_i$ is symmetrically connected to a set of binary hidden units $j$ of state $h_j$; the continuos weight $w_{ij}$ quantifies the strength between units $i$ and $j$. In the training process, a cyclic Gibbs sampling is performed setting the visible units given the hidden ones and vice versa, according to the following probabilities:\n",
    "\n",
    "$$\n",
    "p(h_j=1\\ |\\ \\underline{v}) = \\sigma(b_j + \\sum_iv_iw_{ij})\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1a)\n",
    "$$\n",
    "$$\n",
    "p(v_i=1\\ |\\ \\underline{h}) = \\sigma(a_i + \\sum_jh_jw_{ij})\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1b)\n",
    "$$\n",
    "\n",
    "where $\\sigma(x)=1/(1+e^{-x})$ is the logistic sigmoid function, $a_i$ is the bias of the $i$-th visible unit and $b_j$ is the bias of the $j$-th hidden unit; they act shifting the sigmoid function $\\sigma(x)$. The absence of links among units of the same type simplifies the training process. Moreover, the number of iterations of $(1a)$ and $(1b)$ can be setted to $1$ if real data is used to fix $\\underline{v}$ in the first place.\n",
    "\n",
    "Concerning the training process of RBM, $\\underline{v}$ is setted using real data. Weights $w_{ij}$ are initialized sampling values from a Gaussian distribution of mean $0$ and standard deviation $0.01$. Biases $b_j$ of hidden units are initialized to $0$, although using $-4$ encourages sparsity in the hidden layer, which can be useful for analysis of units activation.\n",
    "\n",
    "Thus $\\underline{h}$ is evaluated by $(1a)$ before passed to $(1b)$ in order to compute back $\\underline{v}$. In such evaluation it is convenient to set $a_i$ to be $\\log[p_i/(1-p_i)]$; fixed the $i$-th visible unit, $p_i$ is defined as the number of times such unit is on over the whole training array set, normalized over the size of the set. Referring to $(1b)$, the choice of shifting the sigmoid $\\sigma(x)$ by the function $a_i=\\log[p_i/(1-p_i)]$ of the average $p_i$ ensures that the units can activate properly, even given initial weights $w_{ij}$ close to $0$.\n",
    "\n",
    "The initialization of biases $a_i$, firstly proposed by Hinton, makes hidden units able to activate and differentiate better their activation based on data, avoiding the risk of getting stuck on values near to $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a07790-159d-40ff-8e5f-1a749217688f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
