{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "**Group 5**:\n",
    "- Elias Maria Bonasera (2149751)\n",
    "- Alberto Casellato (2139206)\n",
    "- Nicola Garbin (2156363)\n",
    "- Francesco Pazzocco (2165861)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 19:31:31.078220: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-03 19:31:31.082641: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-03 19:31:31.094532: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741026691.116051 1246054 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741026691.121754 1246054 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-03 19:31:31.141319: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam, RMSprop, Nadam, SGD\n",
    "from keras_tuner import HyperParameters\n",
    "from keras_tuner import RandomSearch\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "mpl.rc('image', cmap='copper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search of best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        self.xs = None # x standardized\n",
    "        self.xs_train = None\n",
    "        self.xs_valid = None\n",
    "        self.y_train = None\n",
    "        self.y_valid = None\n",
    "        self.tuner = None\n",
    "        self.best_models = None\n",
    "        self.best_hps = None\n",
    "        self.last_fit = None\n",
    "    \n",
    "    def standardize(self):\n",
    "        x_mean = np.mean(self.x, axis=0)\n",
    "        x_std = np.std(self.x, axis=0)\n",
    "\n",
    "        self.xs = (self.x-x_mean)/x_std\n",
    "    \n",
    "    def split(self, percent_training=0.8):\n",
    "        N_train = int( self.xs.shape[0] * percent_training )\n",
    "        self.xs_train, self.y_train = self.xs[:N_train], self.y[:N_train]\n",
    "        self.xs_valid, self.y_valid = self.xs[N_train:], self.y[N_train:]\n",
    "    \n",
    "    def random_search(self, build_model, epochs=3, objective=\"val_accuracy\", \n",
    "                      max_trials=3, executions_per_trial=2, directory=\"results\",\n",
    "                      project_name=\"project\", verbose=1, num_models=3):\n",
    "        \n",
    "        self.tuner = RandomSearch(\n",
    "            hypermodel=build_model,\n",
    "            objective=objective,\n",
    "            max_trials=max_trials,\n",
    "            executions_per_trial=executions_per_trial,\n",
    "            overwrite=True,\n",
    "            directory=directory,\n",
    "            project_name=project_name\n",
    "        )\n",
    "        \n",
    "        self.tuner.search(\n",
    "            self.xs_train, self.y_train, \n",
    "            epochs=epochs, \n",
    "            validation_data=(self.xs_valid, self.y_valid),\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        self.best_models = self.tuner.get_best_models(num_models=num_models)\n",
    "        self.best_hps = self.tuner.get_best_hyperparameters(num_models)\n",
    "    \n",
    "    def fit(self, ix_model=0, epochs=100, batch_size=50, verbose=1):\n",
    "\n",
    "        self.last_fit = self.best_models[ix_model].fit(\n",
    "            self.xs_train, self.y_train,\n",
    "            epochs = epochs, \n",
    "            batch_size = batch_size,\n",
    "            validation_data = (self.xs_valid, self.y_valid),\n",
    "            verbose = verbose\n",
    "        )\n",
    "        \n",
    "    def show_train_vs_valid(self):\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12,5))\n",
    "\n",
    "        # accuracy\n",
    "        axes[0].plot(self.last_fit.history[\"accuracy\"], label=\"train\")\n",
    "        axes[0].plot(self.last_fit.history[\"val_accuracy\"], label=\"valid.\")\n",
    "        axes[0].set_xlabel(\"epoch\")\n",
    "        axes[0].set_ylabel(\"accuracy\")\n",
    "        axes[0].legend()\n",
    "\n",
    "        # loss\n",
    "        axes[1].plot(self.last_fit.history[\"loss\"], label=\"train\")\n",
    "        axes[1].plot(self.last_fit.history[\"val_loss\"], label=\"valid.\")\n",
    "        axes[1].set_xlabel(\"epoch\")\n",
    "        axes[1].set_ylabel(\"loss\")\n",
    "        axes[1].legend()\n",
    "\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    L = 8\n",
    "    \n",
    "    # model\n",
    "    model = Sequential()\n",
    "    model.add( \n",
    "        Dense(\n",
    "            L, input_shape=(L,), \n",
    "            activation=hp.Choice(f\"activation_0\", [\"sigmoid\", \"tanh\", \"relu\", \"elu\", \"leaky_relu\"])\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    for i in range(3):\n",
    "        model.add(\n",
    "            Dense(\n",
    "                units=20, \n",
    "                activation=hp.Choice(f\"activation_{i+1}\", [\"sigmoid\", \"tanh\", \"relu\", \"elu\", \"leaky_relu\"])\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        model.add( Dropout(rate=hp.Choice(f\"rate_{i+1}\", [0., 0.1, 0.2])) )\n",
    "        \n",
    "    model.add( Dense(units=1, activation=\"sigmoid\") )\n",
    "    \n",
    "    # compile\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-6, max_value=1e-1, sampling=\"log\")\n",
    "    optimizers = {\"Adam\": Adam, \"RMSprop\": RMSprop, \"Nesterov\": Nadam, \"SGD\": SGD}\n",
    "    opt_name = hp.Choice(\"optimizer\", list(optimizers.keys()))\n",
    "    model.compile(\n",
    "        optimizer=optimizers[opt_name](learning_rate=learning_rate),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "x = np.loadtxt(\"./DATA/data-for-DNN_type3_L8.dat\", delimiter=\" \")\n",
    "y = np.loadtxt(\"./DATA/labels-for-DNN_type3_L8.dat\", delimiter=\" \")\n",
    "y = y.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 19:31:33.443270: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/alberto/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "network = DNN(x, y)\n",
    "network.standardize()\n",
    "network.split()\n",
    "network.random_search(build_model, epochs=400, max_trials=20,\n",
    "                      executions_per_trial=1, num_models=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.fit(epochs=400, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.show_train_vs_valid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no sign of overfitting since the validation accuracy follows the trend of the training accuracy, and also the validation loss does not increase with the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_acc = []\n",
    "\n",
    "for i in range(len(network.best_models)):\n",
    "    network.fit(ix_model=i, verbose=0)\n",
    "    acc = network.last_fit.history['accuracy'][-1]\n",
    "    list_acc.append(acc)\n",
    "\n",
    "list_ix = []\n",
    "best_acc = max(list_acc)\n",
    "for i, acc in enumerate(list_acc):\n",
    "    if abs(acc - best_acc) < 0.1:\n",
    "        list_ix.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(network, ix_model, k=5):\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    \n",
    "    list_acc = []\n",
    "    for train_index, test_index in kf.split(network.xs):\n",
    "        x_train, x_test = network.xs[train_index], network.xs[test_index]\n",
    "        y_train, y_test = network.y[train_index], network.y[test_index]\n",
    "        network.best_models[ix_model].fit(x_train, y_train, epochs=100, batch_size=50, verbose=0)\n",
    "        acc = network.best_models[ix_model].evaluate(x_test, y_test, verbose=0)\n",
    "        list_acc.append(acc[1])\n",
    "\n",
    "    return np.mean(list_acc), np.std(list_acc)\n",
    "\n",
    "list_mean_acc = []\n",
    "list_std_acc = []\n",
    "best_mean_acc = 0\n",
    "best_ix = 0\n",
    "for i in list_ix:\n",
    "    mean_acc, std_acc = cross_validate(network, i)\n",
    "    list_mean_acc.append(mean_acc)\n",
    "    list_std_acc.append(std_acc)\n",
    "    if mean_acc > best_mean_acc:\n",
    "        best_mean_acc = mean_acc\n",
    "        best_ix = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list_ix, np.array(list_acc)[list_ix], marker='o', color='orange', label='models with similar accuracy', linestyle='None')\n",
    "plt.errorbar(list_ix, list_mean_acc, yerr=list_std_acc, marker='o', capsize=3, color='green', label='mean cross-validation accuracy', linestyle='None')\n",
    "plt.xticks(list(range(len(list_ix))))\n",
    "plt.xlabel('model number')\n",
    "plt.ylabel('accuracy values')\n",
    "plt.legend()\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most accuracy values are high, this implies that the random search was successful in finding good sets of hyperparameters. The presence of green dots near orange dots suggests that these models generalize well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_ix:\n",
    "    print(f\"Model {i}: {network.best_hps[i].values}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the hyperparameters of models with similar accuracy, we can see that Adam is the one that appears the most. SGD does not appear in any of the seleted models.\n",
    "Most layers use ELU, ReLU and Leaky ReLU as activation functions.\n",
    "\n",
    "In the first hidden layer there is for almost every model a dropout rate of $0.1$, and for the second and third layers the most common is a rate of $0$.\n",
    "\n",
    "All learning rates are greater than $10^{-4}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "In this class a data augmentation is performed adding random noise to each point of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C           = [\"L1\", \"L2\", \"L3\", \"L4\", \"L5\", \"L6\", \"L7\", \"L8\"]\n",
    "label       = \"label\"\n",
    "df          = pd.DataFrame(x, columns = C)\n",
    "df[\"label\"] = y\n",
    "#\n",
    "# making plot\n",
    "def making_plot(columns, values, title, xlabel, ylabel):\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    plt.bar(columns, values, color = 'skyblue')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.ylim(0.85, 1)\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    #\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric\n",
    "def metric(df, i, chosen = 1):\n",
    "    tmp = np.array(df[C]) - np.array(df[i:i+1][C])\n",
    "    tmp = abs(tmp).mean(axis = 1)\n",
    "    tmp.sort()\n",
    "    return tmp[chosen] / 2\n",
    "    #\n",
    "#####\n",
    "#\n",
    "# points generator\n",
    "class POINTS(object):\n",
    "    def __init__(self, df):\n",
    "        self.df     = df\n",
    "        self.new_df = None\n",
    "        #\n",
    "    #####\n",
    "    #\n",
    "    def run(self, N = 1, spread = 1):\n",
    "        points = pd.DataFrame([])\n",
    "        for i in range(len(self.df)):\n",
    "            std  = metric(df, i, spread)\n",
    "            th   = (2 * std) / std\n",
    "            new_points = truncnorm.rvs(- 0.75, 0.75, loc = 0, scale = std, size = (N, 8)) + np.array(df[C].iloc[i])\n",
    "            new_points = pd.DataFrame(new_points, columns = C)\n",
    "            new_points[label] = df[label].iloc[i]\n",
    "            points     = pd.concat([points, new_points], ignore_index = True)\n",
    "            #\n",
    "        #####\n",
    "        #\n",
    "        self.new_df = points\n",
    "        #\n",
    "    #####\n",
    "    #\n",
    "    def print(self):\n",
    "        return pd.concat([self.df, self.new_df], ignore_index = True)\n",
    "        #\n",
    "    #####\n",
    "    #\n",
    "#####\n",
    "\n",
    "m = POINTS(df)\n",
    "m.run(N = 2, spread = 2)\n",
    "new_df = m.print()\n",
    "\n",
    "x_new, y_new = np.array(new_df[C]), np.array(new_df[\"label\"])\n",
    "\n",
    "# plotting of new generated and original data\n",
    "fig,  axs = plt.subplots(1, 2, figsize = (15, 7.5))\n",
    "axs[0].scatter(x[:, 0],     x[:, 1],     s = 6, c = y)\n",
    "axs[1].scatter(x_new[:, 0], x_new[:, 1], s = 6, c = y_new)\n",
    "axs[0].set_title(\"Original dataset\")\n",
    "axs[1].set_title(\"Augmented dataset [3x]\")\n",
    "axs[0].set_xlabel(\"L1\")\n",
    "axs[1].set_xlabel(\"L1\")\n",
    "axs[0].set_ylabel(\"L2\")\n",
    "axs[1].set_ylabel(\"L2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing and Increasing\n",
    "In the following a fit is performed both reducing and increasing the size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esit = []\n",
    "\n",
    "for p in [0.1, 0.3, 0.5, 0.7, 0.9, 1]:\n",
    "    if p == 1:\n",
    "        x_r = x\n",
    "        y_r = y\n",
    "    else:\n",
    "        N_r = int(p * len(x))\n",
    "        x_r = x[:N_r]\n",
    "        y_r = y[:N_r]\n",
    "        #\n",
    "    #####\n",
    "    #\n",
    "    network.x = x_r\n",
    "    network.y = y_r\n",
    "    network.standardize()\n",
    "    network.split()\n",
    "    network.fit(ix_model = best_ix, verbose = 0)\n",
    "    esit += [[p, network.last_fit.history['accuracy'][-1]]]\n",
    "    #\n",
    "#####\n",
    "#\n",
    "\n",
    "for p in [1, 2, 3, 4, 9]:\n",
    "    m.run(N = p, spread = 2)\n",
    "    new_df = m.print()\n",
    "    #\n",
    "    x_a = new_df[C].to_numpy()\n",
    "    y_a = new_df[label].to_numpy()\n",
    "    network.x = x_a.astype(float)\n",
    "    network.y = y_a.astype(int)\n",
    "    #\n",
    "    network.standardize()\n",
    "    network.xs_valid, network.y_valid = network.xs[:400], network.y[:400]\n",
    "    network.xs_train, network.y_train = network.xs[400:], network.y[400:]\n",
    "    network.fit(ix_model = best_ix, verbose = 0)\n",
    "    esit += [[p + 1, network.last_fit.history['accuracy'][-1]]]\n",
    "    #\n",
    "#####\n",
    "#\n",
    "\n",
    "esit = np.array(esit).T\n",
    "making_plot([str(p) + \"x\" for p in esit[0]], esit[1], title = \"Accuracy vs Size\", xlabel = \"Size\", ylabel = \"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last graph, we can observe that the model's accuracy tends to decrease as the size of the training set increases beyond 1x, while it remains stable for sizes below the original. This behavior appears to be related to the nature of the growing algorithm: the new generated points are distributed across a neighborhood of each existing point. These fluctuations result in less distinct boundaries between different regions, which seems to negatively impact accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
